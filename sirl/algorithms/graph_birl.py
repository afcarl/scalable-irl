
from __future__ import division

from abc import ABCMeta, abstractmethod

from numpy.random import choice, randint, uniform
import numpy as np

from .mdp_solvers import graph_policy_iteration
from ..base import ModelMixin
from ..utils.common import Logger


__all__ = [
    'GBIRLPolicyWalk',
    'GaussianRewardPrior',
    'LaplacianRewardPrior',
    'UniformRewardPrior',
]


class RewardLoss(object):
    """docstring for RewardLoss"""
    def __init__(self, arg):
        self.arg = arg


########################################################################
# Reward Priors

class RewardPrior(ModelMixin):
    """ Reward prior interface """
    __meta__ = ABCMeta

    def __init__(self, name):
        self.name = name

    @abstractmethod
    def __call__(self, r):
        raise NotImplementedError('Abstract method')

    @abstractmethod
    def log_p(self, r):
        raise NotImplementedError('Abstract method')


class UniformRewardPrior(RewardPrior):
    """ Uniform/flat prior"""
    def __init__(self, name='uniform'):
        super(UniformRewardPrior, self).__init__(name)

    def __call__(self, r):
        return r

    def log_p(self, r):
        return np.log(r)


class GaussianRewardPrior(RewardPrior):
    """Gaussian reward prior"""
    def __init__(self, name='gaussian', sigma=0.5):
        super(GaussianRewardPrior, self).__init__(name)
        self._sigma = sigma

    def __call__(self, r):
        return np.exp(-np.square(r)/(2.0*self._sigma**2)) /\
            np.sqrt(2.0*np.pi)*self._sigma

    def log_p(self, r):
        # TODO - make analytical
        return np.log(self.__call__(r))


class LaplacianRewardPrior(RewardPrior):
    """Laplacian reward prior"""
    def __init__(self, name='laplace', sigma=0.5):
        super(LaplacianRewardPrior, self).__init__(name)
        self._sigma = sigma

    def __call__(self, r):
        return np.exp(-np.fabs(r)/(2.0*self._sigma)) / (2.0*self._sigma)

    def log_p(self, r):
        # TODO - make analytical
        return np.log(self.__call__(r))


########################################################################
# Algorithms

class GBIRL(ModelMixin, Logger):
    """GraphBIRL algorithm

    Bayesian Inverse Reinforcement Learning on Adaptive State-Graph (GBIRL)

    This is an iterative algorithm that improves the reward based on the
    quality differences between expert trajectories and trajectories
    generated by the test rewards

    Parameters
    ----------
    demos : array-like, shape (M x d)
        Expert demonstrations as M trajectories of state action pairs
    mdp : ``GraphMDP`` object
        The underlying (semi) Markov decision problem
    prior : ``RewardPrior`` object
        Reward prior callable
    loss : callable
        Reward loss callable
    max_iter : int, optional (default=10)
        Number of iterations of the GBIRL algorith
    alpha : float, optional (default=0.9)
        Expert optimality parameter for softmax Boltzman temperature


    Attributes
    -----------
    _demos : array-like, shape (M x d)
        Expert demonstrations as M trajectories of state action pairs
    _mdp : ``GraphMDP`` object
        The underlying (semi) Markov decision problem
    _prior : ``RewardPrior`` object
        Reward prior callable
    _loss : callable
        Reward loss callable
    _max_iter : int, optional (default=10)
        Number of iterations of the GBIRL algorith
    _alpha : float, optional (default=0.9)
        Expert optimality parameter for softmax Boltzman temperature

    """

    __meta__ = ABCMeta

    def __init__(self, demos, mdp, prior, loss, alpha=0.7, max_iter=10):
        self._demos = demos
        self._prior = prior
        self._mdp = mdp
        self._loss = loss
        self._alpha = alpha
        self._max_iter = max_iter

    def solve(self):
        """ Find the true reward function """
        reward = self.initialize_reward()
        self._compute_policy(reward=reward)
        init_g_trajs = self._generate_trajestories()
        g_trajs = [init_g_trajs]

        for iteration in range(self._max_iter):
            # - Compute reward likelihood, find the new reward
            result = self.find_next_reward(reward, g_trajs)
            reward = result['reward']

            # - generate trajectories using current reward and store
            self._compute_policy(reward)
            trajs = self._generate_trajestories()
            g_trajs.append(trajs)

            # - compute quality loss over the trajectories
            # TODO

            print('Iteration: {}'.format(iteration))

        return reward

    @abstractmethod
    def find_next_reward(self, g_trajs):
        """ Compute a new reward based on current iteration """
        raise NotImplementedError('Abstract')

    @abstractmethod
    def initialize_reward(self):
        """ Initialize reward function based on sovler """
        raise NotImplementedError('Abstract')

    # -------------------------------------------------------------
    # internals
    # -------------------------------------------------------------

    def _generate_trajestories(self):
        """ Generate trajectories using a given policy """
        self._mdp._find_best_policies()
        return self._mdp._best_trajs

    def _compute_policy(self, reward):
        """ Compute the policy induced by a given reward function """
        # TODO - check that reward is a weight vector
        gea = self._mdp.graph.gea
        sea = self._mdp.graph.sea

        for e in self._mdp.graph.all_edges:
            phi = gea(e[0], e[1], 'phi')
            r = np.dot(phi, reward)
            sea(e[0], e[1], 'reward', r)

        graph_policy_iteration(self._mdp)

    def _expert_trajectory_quality(self, reward, gr=30):
        """ Compute the Q-function of expert trajectories """
        G = self._mdp.graph

        QEs = []
        for traj in self._demos:
            time = 0
            QE = 0
            for n in traj:
                actions = G.out_edges(n)
                if actions:
                    e = actions[G.gna(n, 'pi')]
                    r = np.dot(reward, G.gea(e[0], e[1], 'phi'))
                    QE += (self._mdp.gamma ** time) * r
                    time += G.gea(e[0], e[1], 'duration')
                else:
                    QE += (self._mdp.gamma ** time) * gr
            QEs.append(QE)
        return QEs

    def _generated_trajectory_quality(self, reward, g_trajs, gr=30):
        """ Compute the Q-function of generated trajectories """
        G = self._mdp.graph

        QPiv = []
        for g_traj in g_trajs:
            QPis = []
            for traj in g_traj:
                QPi = 0
                time = 0
                for n in traj:
                    actions = G.out_edges(n)
                    if actions:
                        e = actions[G.gna(n, 'pi')]
                        r = np.dot(reward, G.gea(e[0], e[1], 'phi'))
                        QPi += (self._mdp.gamma ** time) * r
                        time += G.gea(e[0], e[1], 'duration')
                    else:
                        QPi += (self._mdp.gamma ** time) * gr
                QPis.append(QPi)

            QPiv.append(QPis)
        return QPiv


########################################################################

# MCMC proposals

class Proposal(ModelMixin):
    """ Proposal for MCMC sampling """
    def __init__(self, dim):
        self.dim = dim

    @abstractmethod
    def __call__(self, loc):
        raise NotImplementedError('Abstract class')


class PolicyWalkProposal(Proposal):
    """ PolicyWalk MCMC proposal """
    def __init__(self, dim, delta, bounded=True):
        super(PolicyWalkProposal, self).__init__(dim)
        self.delta = delta
        self.bounded = bounded

    def __call__(self, loc):
        new_loc = np.array(loc)
        changed = False
        while not changed:
            d = choice([-self.delta, 0, self.delta])
            i = randint(self.dim)
            if self.bounded:
                if -1 <= new_loc[i]+d <= 1:
                    new_loc[i] += d
                    changed = True
            else:
                new_loc[i] += d
                changed = True
        return new_loc


# PolicyWalk based GraphBIRL


class GBIRLPolicyWalk(GBIRL):
    """GraphBIRL algorithm using PolicyWalk MCMC

    Bayesian Inverse Reinforcement Learning on Adaptive State-Graphs using
    PolicyWalk (GBIRL-PW)

    Reward posterior disctribution is computed using MCMC samples via a
    grid walk on the space of rewards.

    Parameters
    ----------
    demos : array-like, shape (M x d)
        Expert demonstrations as M trajectories of state action pairs
    mdp : ``GraphMDP`` object
        The underlying (semi) Markov decision problem
    prior : ``RewardPrior`` object
        Reward prior callable
    loss : callable
        Reward loss callable
    step_size : float, optional (default=0.2)
        Grid walk step size
    burn : float, optional (default=0.2)
        Fraction of MCMC samples to throw away before the chain stabilizes
    max_iter : int, optional (default=10)
        Number of iterations of the GBIRL algorith
    alpha : float, optional (default=0.9)
        Expert optimality parameter for softmax Boltzman temperature
    reward_max : float, optional (default=1.0)
        Maximum value of the reward signal (for a single dimension)
    mcmc_iter : int, optional (default=200)
        Number of MCMC samples to use in the PolicyWalk algorithm

    Attributes
    -----------
    _delta : float, optional (default=0.2)
        Grid walk step size
    _rmax : float, optional (default=1.0)
        Maximum value of the reward signal (for a single dimension)
    _mcmc_iter : int, optional (default=200)
        Number of MCMC samples to use in the PolicyWalk algorithm
    _burn : float, optional (default=0.2)
        Fraction of MCMC samples to throw away before the chain stabilizes

    Note
    -----
    Using small step sizes generally implies the need for more samples

    """
    def __init__(self, demos, mdp, prior, loss, step_size=1/5.0, burn=0.2,
                 max_iter=10, alpha=0.9, reward_max=1., mcmc_iter=200):
        super(GBIRLPolicyWalk, self).__init__(demos, mdp, prior, loss,
                                              alpha, max_iter)
        self._delta = step_size
        self._rmax = reward_max
        self._mcmc_iter = mcmc_iter
        self._burn = burn

    def initialize_reward(self):
        """
        Generate initial reward for the algorithm in $R^{|S| / \delta}$
        """
        rdim = self._mdp._reward.dim
        v = np.arange(-self._rmax, self._rmax+self._delta, self._delta)
        reward = np.zeros(rdim)
        for i in range(rdim):
            reward[i] = choice(v)
        return reward

    def find_next_reward(self, reward, g_trajs):
        """ Compute a new reward based on current generated trajectories """
        result = dict()
        result['trace'] = []
        result['walk'] = []
        result['reward'] = None
        return self._policy_walk(reward, g_trajs, result)

    # -------------------------------------------------------------
    # internals
    # -------------------------------------------------------------

    def _policy_walk(self, init_reward, g_trajs, result):
        """ Policy Walk MCMC reward posterior computation """
        r = init_reward
        r_mean = np.array(r)
        p_dist = PolicyWalkProposal(r.shape[0], self._delta, bounded=False)

        QE = self._expert_trajectory_quality(r)
        QPi = self._generated_trajectory_quality(r, g_trajs)

        burn_point = int(self._mcmc_iter * self._burn / 100)
        for step in range(1, self._mcmc_iter+1):
            # - generate new reward sample
            r_new = p_dist(loc=r_mean)

            # - Compute new trajectory quality scores
            QE_new = self._expert_trajectory_quality(r_new)
            QPi_new = self._generated_trajectory_quality(r_new, g_trajs)

            # - compute acceptance probability for the new reward
            mh_ratio = self._mh_ratio(r_mean, r_new, QE, QE_new, QPi, QPi_new)
            if uniform(0.0, 1.0) < min([1, mh_ratio]):
                r_mean = self._iterative_reward_mean(r_mean, r_new, step)

            # - handling sample burning
            if step > burn_point:
                result['walk'].append(r_new)
                result['trace'].append(r_mean)
                result['reward'] = r_mean

            # log progress
            if step % 10 == 0:
                print('It: %s, R: %s, R_mean: %s' % (step, r_new, r_mean))
            # self.debug('It: %s, R: %s, R_mean: %s' % (step, r_new, r_mean))

        return result

    def _mh_ratio(self, r, r_new, QE, QE_new, QPi, QPi_new):
        """ Compute the Metropolis-Hastings acceptance ratio

        Given a new reward (weights), MH ratio is used to determine whether or
        not to accept the reward sample.

        Parameters
        -----------
        r : array-like, shape (reward-dim)
            Current reward
        r_new : array-like, shape (reward-dim)
            New reward sample from the MCMC walk
        QE : array-like
            Quality of the expert trajectories based on reward ``r``
        QE_new : array-like
            Quality of the expert trajectories based on reward ``r_new``
        QPi : array-like
            Quality of the generated trajectories based on reward ``r``
        QPi_new : array-like
            Quality of the generated trajectories based on reward ``r_new``

        Returns
        --------
        mh_ratio : float
            The ratio corresponding to,

            .. math::
                ratio = P(R_new|O) / P(R|O) x P(R_new)/P(R)
        """
        # reward priors
        prior_new = sum(self._prior(r_new))
        prior = sum(self._prior(r))

        # print('New ---')
        # print(QPi, QE)

        # likelihoods (un-normalized, since we only need the ratio)
        lk = 1
        for i, Qe in enumerate(QE):
            lk *= np.exp(self._alpha * (Qe)) / \
                  (np.exp(self._alpha * (Qe)) +
                   sum(np.exp(self._alpha * (Qn[i])) for Qn in QPi))

        lk_new = 1
        for i, Qe_new in enumerate(QE_new):
            lk_new *= np.exp(self._alpha * (Qe_new)) / \
                      (np.exp(self._alpha * (Qe_new)) +
                       sum(np.exp(self._alpha * (Qn[i])) for Qn in QPi_new))

        mh_ratio = (lk_new / lk) * (prior_new / prior)
        return mh_ratio

    def _iterative_reward_mean(self, r_mean, r_new, step):
        """ Iterative mean reward

        Compute the iterative mean of the reward using the running mean
        and a new reward sample
        """
        r_mean = [((step - 1) / float(step)) * m_r + 1.0 / step * r
                  for m_r, r in zip(r_mean, r_new)]
        return np.array(r_mean)
