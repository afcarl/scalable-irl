
from __future__ import division
from abc import ABCMeta, abstractmethod

import numpy as np

from ..base import ModelMixin


class RewardLoss(object):
    """docstring for RewardLoss"""
    def __init__(self, arg):
        self.arg = arg


class RewardPrior(object):
    """docstring for RewardPrior"""
    def __init__(self, arg):
        self.arg = arg


class GBIRL(ModelMixin):
    """GraphBIRL algorithm

    This is an iterative algorithm that improves the reward based on the
    quality differences between expert trajectories and trajectories
    generated by the test rewards

    """

    __meta__ = ABCMeta

    def __init__(self, demos, mdp, prior, loss, alpha=0.9, max_iter=10):
        self._demos = demos
        self._prior = prior
        self._mdp = mdp
        self._loss = loss
        self._alpha = alpha
        self._max_iter = max_iter

        self._generated_trajs = []

    def solve(self):
        """ Find the true reward function """

        # - initialize
        e_trajs = self._demos
        reward = self._initial_reward()
        pi0 = self._compute_policy(reward=reward)
        g_trajs = self._generate_trajestories(pi0, size=10)

        for iteration in range(self._max_iter):
            # - Compute reward likelihood, find the new reward
            reward = self.find_next_reward(e_trajs, g_trajs)

            # - generate trajectories using current reward and store
            new_policy = self._compute_policy(reward)
            g_trajs = self._generate_trajestories(new_policy)

        return reward

    @abstractmethod
    def find_next_reward(self, e_trajs, g_trajs):
        """ Compute a new reward based on current iteration """
        raise NotImplementedError('Abstract')

    @abstractmethod
    def initialize_reward(self):
        """ Initialize reward function based on sovler """
        raise NotImplementedError('Abstract')

    def _generate_trajestories(self, policy, size=10):
        """ Generate trajectories using a given policy """
        pass

    def _compute_policy(self, reward):
        """ Compute the policy induced by a given reward function """
        pass


class GBIRLPolicyWalk(GBIRL):
    """GraphBIRL algorithm using PolicyWalk MCMC

    """
    def __init__(self, demos, mdp, prior, loss, step_size=1/5.0,
                 max_iter=10, alpha=0.9, reward_max=1., mcmc_iter=50):
        super(GBIRLPolicyWalk, self).__init__(demos, mdp, prior, loss,
                                              alpha, max_iter)
        self._delta = step_size
        self._rmax = reward_max
        self._mcmc_iter = mcmc_iter

        self._generated_trajs = []
