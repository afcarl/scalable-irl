
from __future__ import division
from abc import ABCMeta, abstractmethod

import numpy as np

from .mdp_solvers import graph_policy_iteration
from ..base import ModelMixin


class RewardLoss(object):
    """docstring for RewardLoss"""
    def __init__(self, arg):
        self.arg = arg


class RewardPrior(object):
    """docstring for RewardPrior"""
    def __init__(self, arg):
        self.arg = arg


class GBIRL(ModelMixin):
    """GraphBIRL algorithm

    This is an iterative algorithm that improves the reward based on the
    quality differences between expert trajectories and trajectories
    generated by the test rewards

    """

    __meta__ = ABCMeta

    def __init__(self, demos, mdp, prior, loss, alpha=0.9, max_iter=10):
        self._demos = demos
        self._prior = prior
        self._mdp = mdp
        self._loss = loss
        self._alpha = alpha
        self._max_iter = max_iter

    def solve(self):
        """ Find the true reward function """

        # - initialize
        e_trajs = self._demos
        reward = self._initial_reward()
        pi0 = self._compute_policy(reward=reward)
        g_trajs = self._generate_trajestories(pi0, size=10)

        for iteration in range(self._max_iter):
            # - Compute reward likelihood, find the new reward
            reward = self.find_next_reward(e_trajs, g_trajs)

            # - generate trajectories using current reward and store
            new_policy = self._compute_policy(reward)
            g_trajs = self._generate_trajestories(new_policy)

        return reward

    @abstractmethod
    def find_next_reward(self, e_trajs, g_trajs):
        """ Compute a new reward based on current iteration """
        raise NotImplementedError('Abstract')

    @abstractmethod
    def initialize_reward(self):
        """ Initialize reward function based on sovler """
        raise NotImplementedError('Abstract')

    # -------------------------------------------------------------
    # internals
    # -------------------------------------------------------------

    def _generate_trajestories(self, policy, size=10):
        """ Generate trajectories using a given policy """
        # TODO - remove the need for arguiments as all info is in graph
        self._mdp._find_best_policies()
        return self._mdp._best_trajs

    def _compute_policy(self, reward):
        """ Compute the policy induced by a given reward function """
        # TODO - check that reward is a weight vector
        gea = self._mdp.graph.gea
        sea = self._mdp.graph.sea

        for e in self._mdp.graph.all_edges:
            phi = gea(e[0], e[1], 'phi')
            r = np.dot(phi, reward)
            sea(e[0], e[1], 'reward', r)

        graph_policy_iteration(self._mdp)
        # TODO - remove the need to return pi as its stored in graph
        policy = self._mdp.graph.policy
        return policy

    def compute_expert_trajectory_quality(self, reward, gr):
        """ Compute the Q-function of expert trajectories """
        QEs = []
        for traj in self._demos:
            time = 0
            QE = 0
            for n in traj:
                if n.edges:  # use python implicit reasoning
                    p = n.pol
                    r = np.dot(reward, n.edges[p].features)
                    QE += (self.mdp.gamma ** time) * r
                    time += n.edges[p].time
                else:  # TODO - fix this (change to check for teminal)
                    QE += (self.mdp.gamma ** time) * gr
            QEs.append(QE)
        return QEs

    def compute_generated_trajectory_quality(self, g_trajs, reward, gr):
        """ Compute the Q-function of generated trajectories """
        QPiv = []
        for g_traj in g_trajs:
            QPis = []
            for traj in g_traj:
                QPi = 0
                time = 0
                for n in traj:
                    if n.edges:  # use python implicit reasoning
                        p = n.pol
                        r = np.dot(reward, n.edges[p].features)
                        QPi += (self.mdp.gamma ** time) * r
                        time += n.edges[p].time
                    else:
                        QPi += (self.mdp.gamma ** time) * gr
                QPis.append(QPi)
            QPiv.append(QPis)
        return QPiv


########################################################################


class GBIRLPolicyWalk(GBIRL):
    """GraphBIRL algorithm using PolicyWalk MCMC

    """
    def __init__(self, demos, mdp, prior, loss, step_size=1/5.0,
                 max_iter=10, alpha=0.9, reward_max=1., mcmc_iter=50):
        super(GBIRLPolicyWalk, self).__init__(demos, mdp, prior, loss,
                                              alpha, max_iter)
        self._delta = step_size
        self._rmax = reward_max
        self._mcmc_iter = mcmc_iter

    def find_next_reward(self, e_trajs, g_trajs):
        """ Compute a new reward based on current iteration using PW """
        pass

    def initialize_reward(self):
        """ Initialize reward function based on sovler """
        pass
