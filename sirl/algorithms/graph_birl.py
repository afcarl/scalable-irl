
from __future__ import division

from abc import ABCMeta, abstractmethod

from numpy.random import choice, randint, uniform
import numpy as np

from .mdp_solvers import graph_policy_iteration
from ..base import ModelMixin
from ..utils.common import Logger


__all__ = [
    'GBIRLPolicyWalk',
    'GaussianRewardPrior',
    'LaplacianRewardPrior',
    'UniformRewardPrior',
]


class RewardLoss(object):
    """docstring for RewardLoss"""
    def __init__(self, arg):
        self.arg = arg


########################################################################
# Reward Priors

class RewardPrior(ModelMixin):
    """ Reward prior interface """
    __meta__ = ABCMeta

    def __init__(self, name):
        self.name = name

    @abstractmethod
    def __call__(self, r):
        raise NotImplementedError('Abstract method')

    @abstractmethod
    def log_p(self, r):
        raise NotImplementedError('Abstract method')


class UniformRewardPrior(RewardPrior):
    """ Uniform/flat prior"""
    def __init__(self, name='uniform'):
        super(UniformRewardPrior, self).__init__(name)

    def __call__(self, r):
        return r

    def log_p(self, r):
        return np.log(r)


class GaussianRewardPrior(RewardPrior):
    """Gaussian reward prior"""
    def __init__(self, name='gaussian', sigma=0.5):
        super(GaussianRewardPrior, self).__init__(name)
        self._sigma = sigma

    def __call__(self, r):
        return np.exp(-np.square(r)/(2.0*self._sigma**2)) /\
            np.sqrt(2.0*np.pi)*self._sigma

    def log_p(self, r):
        # TODO - make analytical
        return np.log(self.__call__(r))


class LaplacianRewardPrior(RewardPrior):
    """Laplacian reward prior"""
    def __init__(self, name='laplace', sigma=0.5):
        super(LaplacianRewardPrior, self).__init__(name)
        self._sigma = sigma

    def __call__(self, r):
        return np.exp(-np.fabs(r)/(2.0*self._sigma)) / (2.0*self._sigma)

    def log_p(self, r):
        # TODO - make analytical
        return np.log(self.__call__(r))


########################################################################
# Algorithms

class GBIRL(ModelMixin, Logger):
    """GraphBIRL algorithm

    This is an iterative algorithm that improves the reward based on the
    quality differences between expert trajectories and trajectories
    generated by the test rewards

    """

    __meta__ = ABCMeta

    def __init__(self, demos, mdp, prior, loss, alpha=0.7, max_iter=10):
        self._demos = demos
        self._prior = prior
        self._mdp = mdp
        self._loss = loss
        self._alpha = alpha
        self._max_iter = max_iter

    def solve(self):
        """ Find the true reward function """

        # - initialize
        reward = self.initialize_reward()
        pi0 = self._compute_policy(reward=reward)
        g_trajs = self._generate_trajestories(pi0, size=10)

        for iteration in range(self._max_iter):
            # - Compute reward likelihood, find the new reward
            result = self.find_next_reward(g_trajs)
            reward = result['reward']

            # - generate trajectories using current reward and store
            new_policy = self._compute_policy(reward)
            g_trajs = self._generate_trajestories(new_policy)

            # - compute quality loss over the trajectories
            # TODO

            print('Iteration: {}'.format(iteration))

        return reward

    @abstractmethod
    def find_next_reward(self, g_trajs):
        """ Compute a new reward based on current iteration """
        raise NotImplementedError('Abstract')

    @abstractmethod
    def initialize_reward(self):
        """ Initialize reward function based on sovler """
        raise NotImplementedError('Abstract')

    # -------------------------------------------------------------
    # internals
    # -------------------------------------------------------------

    def _generate_trajestories(self, policy, size=10):
        """ Generate trajectories using a given policy """
        # TODO - remove the need for arguiments as all info is in graph
        self._mdp._find_best_policies()
        return self._mdp._best_trajs

    def _compute_policy(self, reward):
        """ Compute the policy induced by a given reward function """
        # TODO - check that reward is a weight vector
        gea = self._mdp.graph.gea
        sea = self._mdp.graph.sea

        for e in self._mdp.graph.all_edges:
            phi = gea(e[0], e[1], 'phi')
            r = np.dot(phi, reward)
            sea(e[0], e[1], 'reward', r)

        graph_policy_iteration(self._mdp)
        # TODO - remove the need to return pi as its stored in graph
        policy = self._mdp.graph.policy
        return policy

    def _expert_trajectory_quality(self, reward, gr=50):
        """ Compute the Q-function of expert trajectories """
        G = self._mdp.graph

        QEs = []
        for traj in self._demos:
            time = 0
            QE = 0
            for n in traj:
                actions = G.out_edges(n)
                if actions:
                    e = actions[G.gna(n, 'pi')]
                    r = np.dot(reward, G.gea(e[0], e[1], 'phi'))
                    QE += (self._mdp.gamma ** time) * r
                    time += G.gea(e[0], e[1], 'duration')
                else:
                    QE += (self._mdp.gamma ** time) * gr
            QEs.append(QE)
        return QEs

    def _generated_trajectory_quality(self, reward, g_trajs, gr=50):
        """ Compute the Q-function of generated trajectories """
        G = self._mdp.graph

        # QPiv = []
        # for g_traj in g_trajs:

        QPis = []
        for traj in g_trajs:
            QPi = 0
            time = 0
            for n in traj:
                actions = G.out_edges(n)
                if actions:
                    e = actions[G.gna(n, 'pi')]
                    r = np.dot(reward, G.gea(e[0], e[1], 'phi'))
                    QPi += (self._mdp.gamma ** time) * r
                    time += G.gea(e[0], e[1], 'duration')
                else:
                    QPi += (self._mdp.gamma ** time) * gr
            QPis.append(QPi)

            # QPiv.append(QPis)
        return QPis


########################################################################

# MCMC proposals

class Proposal(ModelMixin):
    """ Proposal for MCMC sampling """
    def __init__(self, dim):
        self.dim = dim

    @abstractmethod
    def __call__(self, loc):
        raise NotImplementedError('Abstract class')


class PolicyWalkProposal(Proposal):
    """ PolicyWalk MCMC proposal """
    def __init__(self, dim, delta, bounded=True):
        super(PolicyWalkProposal, self).__init__(dim)
        self.delta = delta
        self.bounded = bounded

    def __call__(self, loc):
        new_loc = np.array(loc)
        changed = False
        while not changed:
            d = choice([-self.delta, 0, self.delta])
            i = randint(self.dim)
            if self.bounded:
                if -1 <= new_loc[i]+d <= 1:
                    new_loc[i] += d
                    changed = True
            else:
                new_loc[i] += d
                changed = True
        return new_loc


# PolicyWalk based GraphBIRL


class GBIRLPolicyWalk(GBIRL):
    """GraphBIRL algorithm using PolicyWalk MCMC

    """
    def __init__(self, demos, mdp, prior, loss, step_size=1/5.0, burn=0.2,
                 max_iter=10, alpha=0.9, reward_max=1., mcmc_iter=200):
        super(GBIRLPolicyWalk, self).__init__(demos, mdp, prior, loss,
                                              alpha, max_iter)
        self._delta = step_size
        self._rmax = reward_max
        self._mcmc_iter = mcmc_iter
        self._burn = burn

    def initialize_reward(self):
        """
        Generate initial reward for the algorithm in $R^{|S| / \delta}$
        """
        rdim = self._mdp._reward.dim
        v = np.arange(-self._rmax, self._rmax+self._delta, self._delta)
        reward = np.zeros(rdim)
        for i in range(rdim):
            reward[i] = np.random.choice(v)
        return reward

    def find_next_reward(self, g_trajs):
        """ Compute a new reward based on current generated trajectories """
        result = dict()
        result['trace'] = []
        result['walk'] = []
        result['reward'] = None
        return self._policy_walk(g_trajs, result)

    # -------------------------------------------------------------
    # internals
    # -------------------------------------------------------------

    def _policy_walk(self, g_trajs, result):
        """ Policy Walk MCMC reward posterior computation """
        r = self.initialize_reward()
        r_mean = np.array(r)
        proposal_dist = PolicyWalkProposal(dim=r.shape[0],
                                           delta=self._delta,
                                           bounded=False)
        QE = self._expert_trajectory_quality(r, gr=50)
        QPi = self._generated_trajectory_quality(r, g_trajs, gr=50)
        burn_point = int(self._mcmc_iter * self._burn / 100)

        for step in range(1, self._mcmc_iter+1):
            # generate new reward sample
            r_new = proposal_dist(loc=r_mean)

            # Compute new trajectory quality scores
            QE_new = self._expert_trajectory_quality(r_new)
            QPi_new = self._generated_trajectory_quality(r_new, g_trajs)

            # compute acceptance probability for the new reward
            mean_ratio = self._mh_ratio(r_mean, r_new,
                                        QE, QE_new,
                                        QPi, QPi_new)

            # MH accept step
            if uniform(0.0, 1.0) < min([1, mean_ratio]):
                r_mean = self._iterative_reward_mean(r_mean, r_new, step)

            # handling sample burning
            if step > burn_point:
                result['walk'].append(r_new)
                result['trace'].append(r_mean)
                result['reward'] = r_mean

            # log progress
            if step % 10 == 0:
                print('It: %s, R: %s, R_mean: %s' % (step, r_new, r_mean))
            # self.debug('It: %s, R: %s, R_mean: %s' % (step, r_new, r_mean))

        return result

    def _mh_ratio(self, r, r_new, QE, QE_new, QPi, QPi_new):
        """ Compute the Metropolis-Hastings acceptance ratio

        Given a new reward (weights), MH ratio is used to determine whether or
        not to accept the reward sample.

        Parameters
        -----------
        r : array-like, shape (reward-dim)
            Current reward
        r_new : array-like, shape (reward-dim)
            New reward sample from the MCMC walk
        QE : array-like
            Quality of the expert trajectories based on reward ``r``
        QE_new : array-like
            Quality of the expert trajectories based on reward ``r_new``
        QPi : array-like
            Quality of the generated trajectories based on reward ``r``
        QPi_new : array-like
            Quality of the generated trajectories based on reward ``r_new``

        Returns
        --------
        mh_ratio : float
            The ratio corresponding to,

            .. math::
                ratio = P(R_new|O) / P(R|O) x P(R_new)/P(R)
        """
        # reward priors
        prior_new = sum(self._prior(r_new))
        prior = sum(self._prior(r))

        # print('New ---')
        # print(QPi, QE)

        # likelihoods (un-normalized, since we only need the ratio)
        lk = 1
        for i, Qe in enumerate(QE):
            ln = np.exp(self._alpha * Qe)
            lk *= ln / (ln + sum(np.exp(self._alpha * Q) for Q in QPi))

        lk_new = 1
        for i, Qe_new in enumerate(QE_new):
            ln = np.exp(self._alpha * Qe_new)
            lk *= ln / (ln + sum(np.exp(self._alpha * Q) for Q in QPi_new))

        # for i, Qe_new in enumerate(QE_new):
        #     lk_new *= np.exp(self._alpha * (Qe_new)) / \
        #               (np.exp(self._alpha * (Qe_new)) +
        #                sum(np.exp(self._alpha * (Qn[i])) for Qn in QPi_new))

        mh_ratio = (lk_new / lk) * (prior_new / prior)
        return mh_ratio

    def _iterative_reward_mean(self, r_mean, r_new, step):
        """ Iterative mean reward

        Compute the iterative mean of the reward using the running mean
        and a new reward sample
        """
        r_mean = [((step - 1) / float(step)) * m_r + 1.0 / step * r
                  for m_r, r in zip(r_mean, r_new)]
        return np.array(r_mean)
