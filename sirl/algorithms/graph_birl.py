
from __future__ import division

from abc import ABCMeta, abstractmethod

import matplotlib.pyplot as plt
import numpy as np

from .mdp_solvers import graph_policy_iteration
from ..base import ModelMixin
from ..utils.common import Logger


__all__ = [
    'GaussianRewardPrior',
    'LaplacianRewardPrior',
    'UniformRewardPrior',
]


class RewardLoss(ModelMixin):
    """Reward loss function """

    __meta__ = ABCMeta

    def __init__(self, name):
        self.name = name

    @abstractmethod
    def __call__(self, r1, r2):
        """ Reward loss between ``r1`` and ``r2`` """
        raise NotImplementedError('Abstract')


class LpRewardLoss(RewardLoss):
    """L_p reward loss"""
    def __init__(self, p, name='zoe'):
        super(LpRewardLoss, self).__init__(name)
        self.p = p

    def __call__(self, r1, r2):
        return np.linalg.norm(r1=r2, ord=self.p)


########################################################################
# Reward Priors

class RewardPrior(ModelMixin):
    """ Reward prior interface """
    __meta__ = ABCMeta

    def __init__(self, name):
        self.name = name

    @abstractmethod
    def __call__(self, r):
        raise NotImplementedError('Abstract method')

    @abstractmethod
    def log_p(self, r):
        raise NotImplementedError('Abstract method')


class UniformRewardPrior(RewardPrior):
    """ Uniform/flat prior"""
    def __init__(self, name='uniform'):
        super(UniformRewardPrior, self).__init__(name)

    def __call__(self, r):
        return r

    def log_p(self, r):
        return np.log(r)


class GaussianRewardPrior(RewardPrior):
    """Gaussian reward prior"""
    def __init__(self, name='gaussian', sigma=0.5):
        super(GaussianRewardPrior, self).__init__(name)
        self._sigma = sigma

    def __call__(self, r):
        return np.exp(-np.square(r)/(2.0*self._sigma**2)) /\
            np.sqrt(2.0*np.pi)*self._sigma

    def log_p(self, r):
        # TODO - make analytical
        return np.log(self.__call__(r))


class LaplacianRewardPrior(RewardPrior):
    """Laplacian reward prior"""
    def __init__(self, name='laplace', sigma=0.5):
        super(LaplacianRewardPrior, self).__init__(name)
        self._sigma = sigma

    def __call__(self, r):
        return np.exp(-np.fabs(r)/(2.0*self._sigma)) / (2.0*self._sigma)

    def log_p(self, r):
        # TODO - make analytical
        return np.log(self.__call__(r))


########################################################################
# Algorithms

class GBIRL(ModelMixin, Logger):
    """GraphBIRL algorithm

    Bayesian Inverse Reinforcement Learning on Adaptive State-Graph (GBIRL)

    This is an iterative algorithm that improves the reward based on the
    quality differences between expert trajectories and trajectories
    generated by the test rewards

    Parameters
    ----------
    demos : array-like, shape (M x d)
        Expert demonstrations as M trajectories of state action pairs
    mdp : ``GraphMDP`` object
        The underlying (semi) Markov decision problem
    prior : ``RewardPrior`` object
        Reward prior callable
    loss : callable
        Reward loss callable
    max_iter : int, optional (default=10)
        Number of iterations of the GBIRL algorith
    alpha : float, optional (default=0.9)
        Expert optimality parameter for softmax Boltzman temperature


    Attributes
    -----------
    _demos : array-like, shape (M x d)
        Expert demonstrations as M trajectories of state action pairs
    _mdp : ``GraphMDP`` object
        The underlying (semi) Markov decision problem
    _prior : ``RewardPrior`` object
        Reward prior callable
    _loss : callable
        Reward loss callable
    _max_iter : int, optional (default=10)
        Number of iterations of the GBIRL algorith
    _beta : float, optional (default=0.9)
        Expert optimality parameter for softmax Boltzman temperature

    """

    __meta__ = ABCMeta

    def __init__(self, demos, mdp, prior, loss, beta=0.7, max_iter=10):
        self._demos = demos
        self._prior = prior
        self._mdp = mdp
        self._loss = loss
        self._beta = beta
        self._max_iter = max_iter

    def solve(self, persons, relations):
        """ Find the true reward function """
        reward = self.initialize_reward()
        self._compute_policy(reward=reward)
        init_g_trajs = self._generate_trajestories()
        g_trajs = [init_g_trajs]

        self.data = dict()
        self.data['qloss'] = []
        self.data['trace'] = []
        self.data['walk'] = []
        self.data['lk'] = []
        self.data['lk_new'] = []
        self.data['accept_ratios'] = []
        self.data['mh_ratio'] = []

        for iteration in range(self._max_iter):
            # - Compute reward likelihood, find the new reward
            result = self.find_next_reward(reward, g_trajs)
            reward = result['reward']

            self.data['trace'].append(result['trace'])
            self.data['walk'].append(result['walk'])
            self.data['accept_ratios'].append(result['accept_ratio'])
            self.data['mh_ratio'].extend(result['mh_ratio'])

            # - generate trajectories using current reward and store
            self._compute_policy(reward)
            trajs = self._generate_trajestories()
            g_trajs.append(trajs)

            # - compute quality loss over the trajectories
            # TODO

            self._mdp.visualize(persons, relations)
            plt.savefig('learning_{}.pdf'.format(iteration))

            print('Iteration: {}'.format(iteration))

        return reward

    @abstractmethod
    def find_next_reward(self, g_trajs):
        """ Compute a new reward based on current iteration """
        raise NotImplementedError('Abstract')

    @abstractmethod
    def initialize_reward(self):
        """ Initialize reward function based on sovler """
        raise NotImplementedError('Abstract')

    # -------------------------------------------------------------
    # internals
    # -------------------------------------------------------------

    def _generate_trajestories(self):
        """ Generate trajectories using a given policy """
        self._mdp._find_best_policies()
        return self._mdp._best_trajs

    def _compute_policy(self, reward):
        """ Compute the policy induced by a given reward function """
        # TODO - check that reward is a weight vector
        gea = self._mdp.graph.gea
        sea = self._mdp.graph.sea

        for e in self._mdp.graph.all_edges:
            phi = gea(e[0], e[1], 'phi')
            r = np.dot(phi, reward)
            sea(e[0], e[1], 'reward', r)

        graph_policy_iteration(self._mdp)

    def _expert_trajectory_quality(self, reward):
        """ Compute the Q-function of expert trajectories """
        G = self._mdp.graph
        gr = self._mdp._params.goal_reward

        QEs = []
        for traj in self._demos:
            time = 0
            QE = 0
            for n in traj:
                actions = G.out_edges(n)
                if actions:
                    e = actions[G.gna(n, 'pi')]
                    r = np.dot(reward, G.gea(e[0], e[1], 'phi'))
                    QE += (self._mdp.gamma ** time) * r
                    time += G.gea(e[0], e[1], 'duration')
                else:
                    QE += (self._mdp.gamma ** time) * gr
            QEs.append(QE)
        return QEs

    def _generated_trajectory_quality(self, reward, g_trajs):
        """ Compute the Q-function of generated trajectories """
        G = self._mdp.graph
        gr = self._mdp._params.goal_reward

        QPiv = []
        for g_traj in g_trajs:
            QPis = []
            for traj in g_traj:
                QPi = 0
                time = 0
                for n in traj:
                    actions = G.out_edges(n)
                    if actions:
                        e = actions[G.gna(n, 'pi')]
                        r = np.dot(reward, G.gea(e[0], e[1], 'phi'))
                        QPi += (self._mdp.gamma ** time) * r
                        time += G.gea(e[0], e[1], 'duration')
                    else:
                        QPi += (self._mdp.gamma ** time) * gr
                QPis.append(QPi)

            QPiv.append(QPis)
        return QPiv
