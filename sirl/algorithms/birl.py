r"""
Bayesian Inverse Reinforcement Learning using Sampled Trajectories

Appropriate for large and possible infinite state spaces, these algorithms
exploit the learned graph representation (Controller-Graphs) to efficiently
find the underlying reward function

Current implementations:
    1. BIRL Priors
        - GaussianRewardPrior
        - LaplacianRewardPrior
        - UniformRewardPrior
    2. TBIRLOpt -- using L-BFGS to find the reward
    3. TBIRLPolicyWalk -- using policy walk MCMC to find reward
"""

from __future__ import division

from abc import ABCMeta, abstractmethod
from copy import deepcopy
from random import randrange

from scipy.misc import logsumexp
import scipy as sp

import numpy as np
from numpy.random import choice, randint, uniform

from .mdp_solvers import graph_policy_iteration
from ..models.base import ModelMixin
from ..utils.common import Logger


__all__ = [
    'GaussianRewardPrior',
    'LaplacianRewardPrior',
    'UniformRewardPrior',
    'TBIRLOpt',
    'TBIRLPolicyWalk'
]


########################################################################
# Reward Priors

class RewardPrior(ModelMixin):
    """ Reward prior interface """
    __meta__ = ABCMeta

    def __init__(self, name):
        self.name = name

    @abstractmethod
    def __call__(self, r):
        raise NotImplementedError('Abstract method')

    @abstractmethod
    def log_p(self, r):
        raise NotImplementedError('Abstract method')


class UniformRewardPrior(RewardPrior):
    """ Uniform/flat prior"""
    def __init__(self, name='uniform'):
        super(UniformRewardPrior, self).__init__(name)

    def __call__(self, r):
        rp = np.ones(r.shape[0])
        dist = rp / np.sum(rp)
        return dist

    def log_p(self, r):
        return np.log(self.__call__(r))


class GaussianRewardPrior(RewardPrior):
    """Gaussian reward prior"""
    def __init__(self, name='gaussian', sigma=0.5):
        super(GaussianRewardPrior, self).__init__(name)
        self._sigma = sigma

    def __call__(self, r):
        rp = np.exp(-np.square(r)/(2.0*self._sigma**2)) /\
            np.sqrt(2.0*np.pi)*self._sigma
        return rp / np.sum(rp)

    def log_p(self, r):
        # TODO - make analytical
        return np.log(self.__call__(r))


class LaplacianRewardPrior(RewardPrior):
    """Laplacian reward prior"""
    def __init__(self, name='laplace', sigma=0.5):
        super(LaplacianRewardPrior, self).__init__(name)
        self._sigma = sigma

    def __call__(self, r):
        rp = np.exp(-np.fabs(r)/(2.0*self._sigma)) / (2.0*self._sigma)
        return rp / np.sum(rp)

    def log_p(self, r):
        # TODO - make analytical
        return np.log(self.__call__(r))


########################################################################
# Algorithms

class TBIRL(ModelMixin, Logger):
    """Sampled Trajectory based BIRL algorithm (TBIRL)

    Bayesian Inverse Reinforcement Learning on Adaptive State-Graph

    This is an iterative algorithm that improves the reward based on the
    quality differences between expert trajectories and trajectories
    generated by the test rewards

    Parameters
    ----------
    demos : array-like, shape (M x d)
        Expert demonstrations as M trajectories of state action pairs
    mdp : ``GraphMDP`` object
        The underlying (semi) Markov decision problem
    prior : ``RewardPrior`` object
        Reward prior callable
    loss : callable
        Reward loss callable
    max_iter : int, optional (default=10)
        Number of iterations of the TBIRL algorithm
    alpha : float, optional (default=0.9)
        Expert optimality parameter for softmax Boltzman temperature


    Attributes
    -----------
    _demos : array-like, shape (M x d)
        Expert demonstrations as M trajectories of state action pairs
    _prior : ``RewardPrior`` object
        Reward prior callable
    _loss : callable
        Reward loss callable
    _max_iter : int, optional (default=10)
        Number of iterations of the TBIRL algorith
    _beta : float, optional (default=0.9)
        Expert optimality parameter for softmax Boltzman temperature

    """

    __meta__ = ABCMeta

    def __init__(self, demos, cg, prior, loss, beta=0.7, max_iter=10):
        self._demos = demos
        self._prior = prior
        self._rep = cg  # control graph representation
        self._loss = loss
        self._beta = beta
        self._max_iter = max_iter

    def solve(self, persons, relations):
        """ Find the true reward function """
        reward = self.initialize_reward()
        self._compute_policy(reward=reward)
        init_g_trajs = self._rep.find_best_policies()
        g_trajs = [init_g_trajs]

        for iteration in range(self._max_iter):
            # - Compute reward likelihood, find the new reward
            reward = self.find_next_reward(g_trajs)

            # - generate trajectories using current reward and store
            self._compute_policy(reward)
            trajs = self._rep.find_best_policies()
            g_trajs.append(trajs)

            self.info('Iteration: {}'.format(iteration))

        return reward

    @abstractmethod
    def find_next_reward(self, g_trajs):
        """ Compute a new reward based on current iteration """
        raise NotImplementedError('Abstract')

    @abstractmethod
    def initialize_reward(self):
        """ Initialize reward function based on sovler """
        raise NotImplementedError('Abstract')

    # -------------------------------------------------------------
    # internals
    # -------------------------------------------------------------

    def _compute_policy(self, reward):
        """ Compute the policy induced by a given reward function """
        self._rep = self._rep.update_rewards(reward)
        graph_policy_iteration(self._rep.graph,
                               self._rep.mdp.gamma)

    def _expert_trajectory_quality(self, reward):
        """ Compute the Q-function of expert trajectories """
        G = self._rep.graph
        gr = 100  # TODO - make configurable
        gamma = self._rep.mdp.gamma

        QEs = []
        for traj in self._demos:
            time = 0
            QE = 0
            for n in traj:
                actions = G.out_edges(n)
                if actions:
                    e = actions[G.gna(n, 'pi')]
                    r = np.dot(reward, G.gea(e[0], e[1], 'phi'))
                    QE += (gamma ** time) * r
                    time += G.gea(e[0], e[1], 'duration')
                else:
                    QE += (gamma ** time) * gr
            QEs.append(QE)
        return QEs

    def _generated_trajectory_quality(self, reward, g_trajs):
        """ Compute the Q-function of generated trajectories """
        G = self._rep.graph
        gr = 100
        gamma = self._rep.mdp.gamma

        QPiv = []
        for g_traj in g_trajs:
            QPis = []
            for traj in g_traj:
                QPi = 0
                time = 0
                for n in traj:
                    actions = G.out_edges(n)
                    if actions:
                        e = actions[G.gna(n, 'pi')]
                        r = np.dot(reward, G.gea(e[0], e[1], 'phi'))
                        QPi += (gamma ** time) * r
                        time += G.gea(e[0], e[1], 'duration')
                    else:
                        QPi += (gamma ** time) * gr
                QPis.append(QPi)
            QPiv.append(QPis)
        return QPiv


########################################################################


class TBIRLOpt(TBIRL):
    """TBIRL algorithm using direct optimization on the likelihood

    Parameters
    ----------
    demos : array-like, shape (M x d)
        Expert demonstrations as M trajectories of state action pairs
    mdp : ``GraphMDP`` object
        The underlying (semi) Markov decision problem
    prior : ``RewardPrior`` object
        Reward prior callable
    loss : callable
        Reward loss callable
    max_iter : int, optional (default=10)
        Number of iterations of the TBIRL algorithm
    beta : float, optional (default=0.9)
        Expert optimality parameter for softmax Boltzman temperature
    reward_max : float, optional (default=1.0)
        Maximum value of the reward signal (for a single dimension)

    Attributes
    -----------
    _rmax : float, optional (default=1.0)
        Maximum value of the reward signal (for a single dimension)
    _bounds : tuple, optional (default=None)
        Box bounds for L-BFGS optimization of the negative log-likelihood,
        specified for each dimension of the reward function vector, e.g.
        ((-1, 1), (-1, 0)) for a 2D reward vector
    """
    def __init__(self, demos, mdp, prior, loss, max_iter=10, beta=0.9,
                 reward_max=1.0, bounds=None):
        super(TBIRLOpt, self).__init__(demos, mdp, prior, loss,
                                       beta, max_iter)
        self._rmax = reward_max
        self._bounds = bounds
        if self._bounds is None:
            self._bounds = tuple((-self._rmax, self._rmax)
                                 for _ in range(self._rep.mdp.reward.dim))

        self.data = dict()
        self.data['qloss'] = []

    def initialize_reward(self, delta=0.2):
        """
        Generate initial reward
        """
        rdim = self._rep.mdp.reward.dim
        loc = [-self._rmax + i * delta
               for i in range(int(self._rmax / delta + 1))]
        r = [loc[randrange(int(self._rmax / delta + 1))] for _ in range(rdim)]
        reward = np.array(r)
        return reward

    def find_next_reward(self, g_trajs):
        """ Compute a new reward based on current generated trajectories """
        # initialize the reward
        r_init = self.initialize_reward()

        # hack - put the g_trajs a member to avoid passing it to the objective
        self.g_trajs = g_trajs

        # run optimization to minimize N_llk
        objective = self._neg_loglk
        res = sp.optimize.fmin_l_bfgs_b(objective, r_init, approx_grad=1,
                                        bounds=self._bounds)

        print(res)
        reward = res[0]

        return reward

    # -------------------------------------------------------------
    # internals
    # -------------------------------------------------------------

    def _neg_loglk(self, r):
        """ Compute the negative log likelihood with respect to the given
        reward and generated trajectories.
        """
        # - prepare the trajectory quality scores
        QE = self._expert_trajectory_quality(r)
        QPi = self._generated_trajectory_quality(r, self.g_trajs)
        self.data['qloss'].append(self._loss(QE,  QPi))

        # - the negative log likelihood
        z = []
        for q_e in QE:
            for QP_i in QPi:
                for q_i in QP_i:
                    z.append(self._beta*(q_i - q_e))
        lk = logsumexp(z)

        return lk

    def _get_diff_feature_matrix(self, start_state):
        num_g_trajs = len(self.g_trajs)
        time = 0
        rdim = self._rep.mdp.reward.dim
        G = self._rep.graph

        QEf = np.zeros(rdim + 1)
        for n in self._demos[start_state]:
            actions = G.out_edges(n)
            if actions:
                e = actions[G.gna(n, 'pi')]
                tmp = list(G.gea(e[0], e[1], 'phi'))
                tmp.append(0)
                tmp = np.array(tmp)
                time += G.gea(e[0], e[1], 'duration')
                tmp = (self._rep.mdp.gamma ** time) * tmp
                QEf += tmp
                time += G.gea(e[0], e[1], 'duration')
            else:
                tmp = ([0] * rdim)
                tmp.append(1)
                tmp = np.array(tmp)
                tmp = (self._rep.mdp.gamma ** time) * tmp
                QEf += tmp

        Qf = np.zeros((rdim + 1) * num_g_trajs).reshape(rdim + 1, num_g_trajs)
        for i, generated_traj in enumerate(self.g_trajs):
            QPif = np.zeros(rdim + 1)
            time = 0
            for n in generated_traj[start_state]:
                if n.get_edges() != []:
                    e = actions[G.gna(n, 'pi')]
                    tmp = list(G.gea(e[0], e[1], 'phi'))
                    tmp.append(0)
                    tmp = np.array(tmp)
                    tmp = (self._rep.mdp.gamma ** time) * tmp
                    QPif += tmp
                    time += G.gea(e[0], e[1], 'duration')
                else:
                    tmp = ([0] * rdim)
                    tmp.append(1)
                    tmp = np.array(tmp)
                    tmp = (self._rep.mdp.gamma ** time) * tmp
                    QPif += tmp
            Qf[:, i] = np.transpose(QPif - QEf)
        return Qf

    def _ais(self, start_state, r):
        goal_reward = 100
        G = self._rep.graph
        time = 0
        QE = 0
        for n in self._demos[start_state]:
            actions = G.out_edges(n)
            if actions:
                e = actions[G.gna(n, 'pi')]
                r = np.dot(r, G.gea(e[0], e[1], 'phi'))
                QE += (self._rep.mdp.gamma ** time) * r
                time += G.gea(e[0], e[1], 'duration')
            else:
                QE += (self._rep.mdp.gamma ** time) * goal_reward

        QPis = []
        for generated_traj in self.g_trajs:
            QPi = 0
            time = 0
            for n in generated_traj[start_state]:
                actions = G.out_edges(n)
                if actions:
                    e = actions[G.gna(n, 'pi')]
                    r = np.dot(r, G.gea(e[0], e[1], 'phi'))
                    QE += (self._rep.mdp.gamma ** time) * r
                    time += G.gea(e[0], e[1], 'duration')
                else:
                    QPi += (self._rep.mdp.gamma ** time) * goal_reward
            QPis.append(QPi)
        QPis = [np.exp(Q - QE) for Q in QPis]
        tot = sum(Q for Q in QPis)
        QPis = [Q / float(1 + tot) for Q in QPis]
        QPis = np.array(QPis)
        return QPis

    def _grad_nloglk(self, r):
        """ Gradient of the negative log likelihood
        """
        num_starts = len(self._demos)
        grad = sum(np.mat(self._get_diff_feature_matrix(i)) *
                   np.transpose(np.mat(self._ais(i, r)))
                   for i in range(num_starts))
        return grad


########################################################################

# MCMC proposals

class Proposal(ModelMixin):
    """ Proposal for MCMC sampling """
    __meta__ = ABCMeta

    def __init__(self, dim):
        self.dim = dim

    @abstractmethod
    def __call__(self, loc):
        raise NotImplementedError('Abstract class')


class PolicyWalkProposal(Proposal):
    """ PolicyWalk MCMC proposal """
    def __init__(self, dim, delta, bounded=True):
        super(PolicyWalkProposal, self).__init__(dim)
        self.delta = delta
        self.bounded = bounded
        # TODO - allow setting bounds as list of arrays

    def __call__(self, loc):
        new_loc = np.array(loc)
        changed = False
        while not changed:
            d = choice([-self.delta, self.delta])
            i = randint(self.dim)
            if self.bounded:
                if -1 <= new_loc[i]+d <= 1:
                    new_loc[i] += d
                    changed = True
            else:
                new_loc[i] += d
                changed = True
        return new_loc


# PolicyWalk


class TBIRLPolicyWalk(TBIRL):
    """GraphBIRL algorithm using PolicyWalk MCMC

    Bayesian Inverse Reinforcement Learning on Adaptive State-Graphs using
    PolicyWalk (TBIRL-PW)

    Reward posterior disctribution is computed using MCMC samples via a
    grid walk on the space of rewards.

    Parameters
    ----------
    demos : array-like, shape (M x d)
        Expert demonstrations as M trajectories of state action pairs
    mdp : ``GraphMDP`` object
        The underlying (semi) Markov decision problem
    prior : ``RewardPrior`` object
        Reward prior callable
    loss : callable
        Reward loss callable
    step_size : float, optional (default=0.2)
        Grid walk step size
    burn : float, optional (default=0.2)
        Fraction of MCMC samples to throw away before the chain stabilizes
    max_iter : int, optional (default=10)
        Number of iterations of the TBIRL algorithm
    beta : float, optional (default=0.9)
        Expert optimality parameter for softmax Boltzman temperature
    reward_max : float, optional (default=1.0)
        Maximum value of the reward signal (for a single dimension)
    mcmc_iter : int, optional (default=200)
        Number of MCMC samples to use in the PolicyWalk algorithm

    Attributes
    -----------
    _delta : float, optional (default=0.2)
        Grid walk step size
    _rmax : float, optional (default=1.0)
        Maximum value of the reward signal (for a single dimension)
    _mcmc_iter : int, optional (default=200)
        Number of MCMC samples to use in the PolicyWalk algorithm
    _burn : float, optional (default=0.2)
        Fraction of MCMC samples to throw away before the chain stabilizes

    Note
    -----
    Using small step sizes generally implies the need for more samples

    """
    def __init__(self, demos, cg, prior, loss, step_size=0.3, burn=0.2,
                 max_iter=10, beta=0.9, reward_max=1.0, mcmc_iter=200,
                 cooling=False):
        super(TBIRLPolicyWalk, self).__init__(demos, cg, prior, loss,
                                              beta, max_iter)
        self._delta = step_size
        self._rmax = reward_max
        self._mcmc_iter = mcmc_iter
        self._burn = burn
        self._tempered = cooling

        # some data for diagnosis
        self.data = dict()
        self.data['qloss'] = []
        self.data['trace'] = []
        self.data['walk'] = []
        self.data['accept_ratios'] = []
        self.data['iter_rewards'] = []

    def initialize_reward(self):
        """
        Generate initial reward for the algorithm in $R^{|S| / \delta}$
        """
        rdim = self._rep.mdp.reward.dim
        loc = [-self._rmax + i * self._delta
               for i in range(int(self._rmax / self._delta + 1))]
        r = [loc[randrange(int(self._rmax / self._delta + 1))]
             for _ in range(rdim)]
        reward = np.array(r)
        if self._tempered:
            # initialize to the maximum of prior
            prior = self._prior(reward)
            reward = np.array([max(prior) for _ in range(rdim)])

        return reward

    def find_next_reward(self, g_trajs):
        """ Compute a new reward based on current generated trajectories """
        return self._policy_walk(g_trajs)

    # -------------------------------------------------------------
    # internals
    # -------------------------------------------------------------

    def _policy_walk(self, g_trajs):
        """ Policy Walk MCMC reward posterior computation """
        r = self.initialize_reward()
        r_mean = deepcopy(r)
        p_dist = PolicyWalkProposal(r.shape[0], self._delta, bounded=True)

        QE = self._expert_trajectory_quality(r)
        QPi = self._generated_trajectory_quality(r, g_trajs)
        ql = sum([sum(Qe - Qp for Qe, Qp in zip(QE, Q_i)) for Q_i in QPi])
        self.data['qloss'].append(ql)
        burn_point = int(self._mcmc_iter * self._burn / 100)

        for step in range(1, self._mcmc_iter+1):
            r_new = p_dist(loc=r_mean)
            QE_new = self._expert_trajectory_quality(r_new)
            QPi_new = self._generated_trajectory_quality(r_new, g_trajs)

            mh_ratio = self._mh_ratio(r_mean, r_new, QE, QE_new, QPi, QPi_new)
            accept_probability = min(1, mh_ratio)
            if self._tempered:
                accept_probability = min(1, mh_ratio) ** self._cooling(step)

            if accept_probability > uniform(0, 1):
                r_mean = self._iterative_reward_mean(r_mean, r_new, step)
                self.data['accept_ratios'].append(1)

            # - handling sample burning
            if step > burn_point:
                self.data['trace'].append(r_mean)
                self.data['walk'].append(r_new)

            if step % 10 == 0:
                print('It: %s, R: %s, R_mean: %s' % (step, r_new, r_mean))
            # self.debug('It: %s, R: %s, R_mean: %s' % (step, r_new, r_mean))

        self.data['iter_rewards'].append(r_mean)
        return r_mean

    def _mh_ratio(self, r, r_new, QE, QE_new, QPi, QPi_new):
        """ Compute the Metropolis-Hastings acceptance ratio

        Given a new reward (weights), MH ratio is used to determine whether or
        not to accept the reward sample.

        Parameters
        -----------
        r : array-like, shape (reward-dim)
            Current reward
        r_new : array-like, shape (reward-dim)
            New reward sample from the MCMC walk
        QE : array-like
            Quality of the expert trajectories based on reward ``r``
        QE_new : array-like
            Quality of the expert trajectories based on reward ``r_new``
        QPi : array-like
            Quality of the generated trajectories based on reward ``r``
        QPi_new : array-like
            Quality of the generated trajectories based on reward ``r_new``

        Returns
        --------
        mh_ratio : float
            The ratio corresponding to :math:`P(r_n|O) / P(r|O) x P(r_n)/P(r)`
        """
        # - initialize reward posterior distribution to log priors
        p_new = np.sum(self._prior.log_p(r_new))
        p = np.sum(self._prior.log_p(r))

        # - log-likelihoods
        z = []
        for q_e in QE:
            for QP_i in QPi:
                for q_i in QP_i:
                    z.append(self._beta*(q_i - q_e))
        lk = -logsumexp(z)

        z_new = []
        for q_e_new in QE_new:
            for QP_i_new in QPi_new:
                for q_i_new in QP_i_new:
                    z_new.append(self._beta*(q_i_new - q_e_new))
        lk_new = -logsumexp(z_new)

        mh_ratio = (lk_new + p_new) / (lk + p)
        return mh_ratio

    def _iterative_reward_mean(self, r_mean, r_new, step):
        """ Iterative mean reward

        Compute the iterative mean of the reward using the running mean
        and a new reward sample
        """
        r_mean = [((step - 1) / float(step)) * m_r + 1.0 / step * r
                  for m_r, r in zip(r_mean, r_new)]
        return np.array(r_mean)

    def _cooling(self, step):
        """ Tempering """
        return 5 + step / 50.0
